{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor\n",
    "import sklearn.grid_search as gs\n",
    "import xgboost as xgb\n",
    "import h2o\n",
    "import time\n",
    "import pylab as pl\n",
    "import matplotlib.font_manager\n",
    "from scipy import stats\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/train.csv')\n",
    "store = pd.read_csv('Data/store.csv', index_col='Store')\n",
    "test = pd.read_csv('Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ToWeight(y):\n",
    "    w = np.zeros(y.shape, dtype=float)\n",
    "    ind = y != 0\n",
    "    w[ind] = 1./(y[ind]**2)\n",
    "    return w\n",
    "\n",
    "\n",
    "def rmspe(yhat, y):\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    # y = y.values\n",
    "    y = y.get_label()\n",
    "    y = np.exp(y) - 1\n",
    "    yhat = np.exp(yhat) - 1\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean(w * (y - yhat)**2))\n",
    "    return \"rmspe\", rmspe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data modifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def type_to_numeric(x):\n",
    "    if(x == \"a\"):\n",
    "        return 1\n",
    "    elif(x == \"b\"):\n",
    "        return 2\n",
    "    elif(x == \"c\"):\n",
    "        return 3\n",
    "    elif(x == \"d\"):\n",
    "        return 4\n",
    "    \n",
    "def assort_to_numeric(x):\n",
    "    if(x == \"a\"):\n",
    "        return 1\n",
    "    elif(x == \"b\"):\n",
    "        return 2\n",
    "    elif(x == \"c\"):\n",
    "        return 3\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train = train.loc[(train.Open == 1) & (train.Sales != 0)]\n",
    "train = train.loc[train.Open == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train[\"StoreType\"] = 1\n",
    "train[\"Assortment\"] = 1\n",
    "\n",
    "train_final = pd.DataFrame()\n",
    "train_final[\"Open\"] = train.Open\n",
    "train_final[\"Promo\"] = train.Promo\n",
    "train_final[\"Store\"] = train.Store\n",
    "train_final[\"DayOfWeek\"] = train.DayOfWeek\n",
    "train_final[\"CompDist\"] = 1\n",
    "train_final[\"Day\"] = train.Date.apply(lambda x: int(x.split('-')[2]))\n",
    "train_final[\"Month\"] = train.Date.apply(lambda x: int(x.split('-')[1]))\n",
    "train_final[\"Year\"] = train.Date.apply(lambda x: int(x.split('-')[0]))\n",
    "\n",
    "test[\"StoreType\"] = 1\n",
    "test[\"Assortment\"] = 1\n",
    "\n",
    "test_final = pd.DataFrame()\n",
    "test_final[\"Open\"] = test.Open\n",
    "test_final[\"Promo\"] = test.Promo\n",
    "test_final[\"Store\"] = test.Store\n",
    "test_final[\"DayOfWeek\"] = test.DayOfWeek\n",
    "test_final[\"CompDist\"] = 1\n",
    "test_final[\"Day\"] = test.Date.apply(lambda x: int(x.split('-')[2]))\n",
    "test_final[\"Month\"] = test.Date.apply(lambda x: int(x.split('-')[1]))\n",
    "test_final[\"Year\"] = test.Date.apply(lambda x: int(x.split('-')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.loc[:,[\"Assortment\", \"StoreType\"]] = store.loc[train.Store,{\"StoreType\", \"Assortment\"}].values\n",
    "train.loc[:,\"CompDist\"] = store.loc[train.Store, \"CompetitionDistance\"].values\n",
    "train.CompDist = train.CompDist.fillna(0)\n",
    "\n",
    "test.loc[:,[\"Assortment\", \"StoreType\"]] = store.loc[test.Store,{\"StoreType\", \"Assortment\"}].values\n",
    "test.loc[:,\"CompDist\"] = store.loc[test.Store, \"CompetitionDistance\"].values\n",
    "test.CompDist = test.CompDist.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(x = train.CompDist)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(x = train.CompDist.apply(lambda x: math.log(x, 2) if x != 0 else 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_final[\"LogCompDist\"] = train.CompDist.apply(lambda x: math.log(x) if x != 0 else 0)\n",
    "test_final[\"LogCompDist\"] = test.CompDist.apply(lambda x: math.log(x) if x != 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(train.Sales, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thay aren't normaly distributed, so try to take a log value of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(train.Sales.apply(lambda x: math.log(x + 1, 2)), bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_final.loc[:, \"SalesLog\"] = train.Sales.apply(lambda x: math.log(x, 2))\n",
    "train_final.loc[:, \"Sales\"] = train.Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_final[\"StoreA\"] = train.StoreType.apply(lambda x: 1 if x == 'a' else 0)\n",
    "train_final[\"StoreB\"] = train.StoreType.apply(lambda x: 1 if x == 'b' else 0)\n",
    "train_final[\"StoreC\"] = train.StoreType.apply(lambda x: 1 if x == 'c' else 0)\n",
    "train_final[\"StoreD\"] = train.StoreType.apply(lambda x: 1 if x == 'd' else 0)\n",
    "train_final[\"AssortA\"] = train.StoreType.apply(lambda x: 1 if x == 'a' else 0)\n",
    "train_final[\"AssortB\"] = train.StoreType.apply(lambda x: 1 if x == 'b' else 0)\n",
    "train_final[\"AssortC\"] = train.StoreType.apply(lambda x: 1 if x == 'c' else 0)\n",
    "\n",
    "test_final[\"StoreA\"] = test.StoreType.apply(lambda x: 1 if x == 'a' else 0)\n",
    "test_final[\"StoreB\"] = test.StoreType.apply(lambda x: 1 if x == 'b' else 0)\n",
    "test_final[\"StoreC\"] = test.StoreType.apply(lambda x: 1 if x == 'c' else 0)\n",
    "test_final[\"StoreD\"] = test.StoreType.apply(lambda x: 1 if x == 'd' else 0)\n",
    "test_final[\"AssortA\"] = test.StoreType.apply(lambda x: 1 if x == 'a' else 0)\n",
    "test_final[\"AssortB\"] = test.StoreType.apply(lambda x: 1 if x == 'b' else 0)\n",
    "test_final[\"AssortC\"] = test.StoreType.apply(lambda x: 1 if x == 'c' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I know, there is some **NaN** values in few columns in test table, let's check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(train[(train.Sales == 0) & (train.Open == 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(train.loc[train_final.CompDist == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tmp = train.copy()\n",
    "test_tmp = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_final[\"SchoolHoliday\"] = train.SchoolHoliday\n",
    "test_final[\"SchoolHoliday\"] = train.SchoolHoliday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_final[\"YearWeek\"] = train.Date.apply(lambda x:\n",
    "                                           int(time.strftime(\"%W\", time.strptime(x, \"%Y-%m-%d\"))))\n",
    "test_final[\"YearWeek\"] = test.Date.apply(lambda x:\n",
    "                                           int(time.strftime(\"%W\", time.strptime(x, \"%Y-%m-%d\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_final.copy()\n",
    "test = test_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = test.fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train[\"LogSales\"] = np.log(train[\"Sales\"] + 1)\n",
    "# train.to_csv('Data/ready_data_cleared_zero_sales_1.csv', index=False)\n",
    "train.to_csv('Data/ready_data_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test[\"Id\"] = test_tmp.Id\n",
    "test.to_csv('Data/ready_test_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's detect and remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Data/ready_data_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Init of the resulting Dataframe\n",
    "#result = pd.DataFrame([1])\n",
    "\n",
    "# Example settings\n",
    "def remove_outliers(train, outliers_fraction = 0.01):\n",
    "    print __doc__\n",
    "    clusters_separation = range(1, 13)\n",
    "\n",
    "    clf = svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05, kernel=\"rbf\", gamma=0.1)\n",
    "\n",
    "    low_bound_1 = -1\n",
    "    low_bound_2 = -1\n",
    "    up_bound_1 = np.max(train.Store) + 1\n",
    "    up_bound_2 = np.max(train.LogSales) + 7\n",
    "    xx, yy = np.meshgrid(np.linspace(low_bound_1, up_bound_2, 500), np.linspace(low_bound_2, up_bound_2, 500))\n",
    "\n",
    "    # Fit the problem with varying cluster separation\n",
    "    for i, month in enumerate(clusters_separation):\n",
    "        #np.random.seed(42)\n",
    "        #X = train[train.Month == month].loc[:, [\"Store\", \"LogSales\"]].values\n",
    "        tmp = train[train.Month == month].copy()\n",
    "        #X = tmp.loc[:, [\"Store\", \"LogSales\"]].values\n",
    "        X = tmp.loc[:, [\"LogSales\", \"LogSales\"]].values\n",
    "\n",
    "        pl.figure(figsize=(10, 5))\n",
    "        pl.set_cmap(pl.cm.Blues_r)\n",
    "        clf.fit(X)\n",
    "        y_pred = clf.decision_function(X).ravel()\n",
    "        threshold = stats.scoreatpercentile(y_pred,\n",
    "                                            100 * outliers_fraction)\n",
    "        y_pred = y_pred > threshold\n",
    "        if (i == 0):\n",
    "            result = tmp[y_pred].copy()\n",
    "        else:\n",
    "            result = result.append(tmp[y_pred == 1])\n",
    "\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        subplot = pl.subplot(1, 1, 1)\n",
    "        subplot.set_title(\"Outlier detection\")\n",
    "        subplot.contourf(xx, yy, Z,\n",
    "                         levels=np.linspace(Z.min(), threshold, 7))\n",
    "        a = subplot.contour(xx, yy, Z, levels=[threshold],\n",
    "                            linewidths=2, colors='red')\n",
    "        subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],\n",
    "                         colors='orange')\n",
    "        b = subplot.scatter(X[:, 0], X[:, 1], c='black')\n",
    "        subplot.axis('tight')\n",
    "        subplot.legend(\n",
    "            [a.collections[0], b],\n",
    "            ['learned decision function', 'samples'],\n",
    "            prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "        subplot.set_xlim((low_bound_1, up_bound_2))\n",
    "        subplot.set_ylim((low_bound_2, up_bound_2))\n",
    "\n",
    "        pl.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)\n",
    "#         print i\n",
    "\n",
    "    pl.show()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.to_csv(\"Data/ready_data_without_outliers_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Data/ready_data_without_outliers_1.csv\")\n",
    "test = pd.read_csv(\"Data/ready_test_1.csv\")\n",
    "test = First_level_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_model_gen(train, features, n_est = 1100, lambda_ = 0.5, eta = 0.5):\n",
    "    params = {\"objective\": \"reg:linear\",\n",
    "              \"booster\" : \"gbtree\",\n",
    "              \"eta\": eta, #0.025, 0.3\n",
    "              \"max_depth\": 8, #8 \n",
    "              \"subsample\": 0.8, #0.7\n",
    "              \"colsample_bytree\": 0.7,\n",
    "              \"silent\": 1,\n",
    "              \"seed\" : 213,\n",
    "              \"lambda\" : lambda_,\n",
    "              \"alpha\" : 0.0\n",
    "              }\n",
    "    num_trees = n_est\n",
    "    val_size = 100000\n",
    "    X_train, X_test = cross_validation.train_test_split(train, test_size=0.01)\n",
    "    dtrain = xgb.DMatrix(X_train[features], X_train[\"LogSales\"])\n",
    "    dvalid = xgb.DMatrix(X_test[features], X_test[\"LogSales\"])\n",
    "    watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "    xgb_mod = xgb.train(params, dtrain, num_trees, evals=watchlist, \n",
    "                    early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=True)\n",
    "    return xgb_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_res = 1\n",
    "for n_est in [300, 700, 900, 1100]:\n",
    "    error = 0\n",
    "    for i in range(4):\n",
    "        params = {\"objective\": \"reg:linear\",\n",
    "                  \"booster\" : \"gbtree\",\n",
    "                  \"eta\": 0.5, #0.025, 0.3\n",
    "                  \"max_depth\": 8, #8 \n",
    "                  \"subsample\": 0.8, #0.7\n",
    "                  \"colsample_bytree\": 0.7,\n",
    "                  \"silent\": 1,\n",
    "                  \"seed\" : 213,\n",
    "                  \"lambda\" : 0.5,\n",
    "                  \"alpha\" : 0.0\n",
    "                  }\n",
    "        num_trees = n_est\n",
    "\n",
    "        print(\"Train a XGBoost model\")\n",
    "        val_size = 100000\n",
    "        X_train, X_test = cross_validation.train_test_split(train, test_size=0.01)\n",
    "        dtrain = xgb.DMatrix(X_train[features], np.log(X_train[\"Sales\"] + 1))\n",
    "        dvalid = xgb.DMatrix(X_test[features], np.log(X_test[\"Sales\"] + 1))\n",
    "        dtest = xgb.DMatrix(test[features])\n",
    "        watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "        gbm = xgb.train(params, dtrain, num_trees, evals=watchlist, \n",
    "                        early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=True)\n",
    "\n",
    "#             print(\"Validating\")\n",
    "        train_probs = gbm.predict(xgb.DMatrix(X_test[features]))\n",
    "        indices = train_probs < 0\n",
    "        train_probs[indices] = 0\n",
    "        error = error + rmspe(np.exp(train_probs) - 1, X_test['Sales'].values)\n",
    "#         print('error', error)\n",
    "\n",
    "#         print(\"Make predictions on the test set\")\n",
    "#         predictions = gbm.predict(xgb.DMatrix(test[features]))\n",
    "\n",
    "#         predictions = h2o_rf.predict(h2o_test[features])\n",
    "    error = error / 4\n",
    "    commit = \"For used n_est = \" + str(n_est) + \"\\nError = \" + str(error)\n",
    "    if (error < best_res):\n",
    "        best_res = error\n",
    "        best_commit = commit\n",
    "    print commit\n",
    "# print \"Rmse = \" + str(res)\n",
    "print \"===============================\"\n",
    "print best_commit\n",
    "# print best_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For used eta = 0.3, n_est = 500\n",
    "Error = 0.113927837496\n",
    "For eta = 0.5 n_est = 700\n",
    "Error ~ 0.106...\n",
    "For used eta = 0.5, n_est = 700\n",
    "Error = 0.0928495165322\n",
    "For used subsample = 0.8, maxdepth = 8\n",
    "Error = 0.0907461244311\n",
    "For used alpha = 0.0, lambda = 0.5\n",
    "Error = 0.0907181528501\n",
    "For used n_est = 1100\n",
    "Error = 0.0902104248579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['Open', 'Promo', 'Store', 'DayOfWeek', 'CompDist', 'Day',\n",
    "       'Month', 'Year', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "       'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC']\n",
    "\n",
    "# print(\"augment features\")\n",
    "# build_features(features, train)\n",
    "# build_features([], test)\n",
    "# print(features)\n",
    "def xgboost(train, test, features, seed = 213, eta = 0.3, n_est = 300):\n",
    "# My params\n",
    "    params = {\"objective\": \"reg:linear\",\n",
    "              \"eta\": eta, #0.025, 0.3\n",
    "              \"max_depth\": 8,\n",
    "              \"subsample\": 0.7,\n",
    "              \"colsample_bytree\": 0.7,\n",
    "              \"silent\": 1,\n",
    "              \"seed\" : seed \n",
    "              }\n",
    "#     params = {\"objective\" : \"reg:linear\", \n",
    "#                \"booster\" : \"gbtree\",\n",
    "#                \"eta\" : 0.02, # 0.06, #0.01,\n",
    "#                \"max_depth\" : 10, #changed from default of 8\n",
    "#                \"subsample\" : 0.9, # 0.7\n",
    "#                \"colsample_bytree\" : 0.7,\n",
    "#                \"seed\" : seed}\n",
    "    num_trees = n_est\n",
    "\n",
    "    print(\"Train a XGBoost model\")\n",
    "    val_size = 100000\n",
    "    X_train, X_test = cross_validation.train_test_split(train, test_size=0.01)\n",
    "    dtrain = xgb.DMatrix(X_train[features], np.log(X_train[\"Sales\"] + 1))\n",
    "    dvalid = xgb.DMatrix(X_test[features], np.log(X_test[\"Sales\"] + 1))\n",
    "    dtest = xgb.DMatrix(test[features])\n",
    "    watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "    gbm = xgb.train(params, dtrain, num_trees, evals=watchlist, \n",
    "                    early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=True)\n",
    "\n",
    "    print(\"Validating\")\n",
    "    train_probs = gbm.predict(xgb.DMatrix(X_test[features]))\n",
    "    indices = train_probs < 0\n",
    "    train_probs[indices] = 0\n",
    "    error = rmspe(np.exp(train_probs) - 1, X_test['Sales'].values)\n",
    "    print('error', error)\n",
    "\n",
    "    print(\"Make predictions on the test set\")\n",
    "    test_probs = gbm.predict(xgb.DMatrix(test[features]))\n",
    "    indices = test_probs < 0\n",
    "    test_probs[indices] = 0\n",
    "    indices = test.Open == 0\n",
    "    test_probs[indices] = 0\n",
    "    submission = pd.DataFrame({\"Id\": test[\"Id\"], \"Sales\": np.exp(test_probs) - 1})\n",
    "    return submission\n",
    "# submission.to_csv(\"Goose_XGBoost.csv\", index=False)\n",
    "# submission.Sales = submission.Sales * 0.985\n",
    "# submission.to_csv(\"Goose_XGBoost_hint.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest (h2o)\n",
    "Even after some **Native Grid Search** it scores **0.23619** using **400** estimators.   \n",
    "Don't think that we need to continue experiment with it cause results are not good at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test = cross_validation.train_test_split(train, test_size=0.01)\n",
    "X_train.to_csv(\"tmp/data_1.csv\")\n",
    "X_test.to_csv(\"tmp/test_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('Data/ready_test_1.csv')\n",
    "features = ['Open', 'Store', 'DayOfWeek', 'YearWeek', 'Promo', 'CompDist', 'Day',\n",
    "       'Month', 'Year', 'SchoolHoliday', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "       'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC']\n",
    "h2o.init(start_h2o = True)\n",
    "h2o_train = h2o.upload_file(path = \"/Users/Alimantu/Documents/Python/RossmanStoreSales/tmp/data_1.csv\")\n",
    "#h2o_train.describe()\n",
    "#h2o_dl = h2o.deeplearning(x = h2o_train[features], y = h2o_train[\"LogSales\"])\n",
    "bton = True;\n",
    "bdt = True;\n",
    "best_res = 1;\n",
    "for mabs in [0.5, 1, 3, 5, 7]:\n",
    "    for bc in [False, True]:\n",
    "        for md in [10, 15, 20]:\n",
    "            h2o_rf = h2o.h2o.random_forest(x = h2o_train[features],\n",
    "                                           y = h2o_train[\"LogSales\"],  \n",
    "                                           training_frame=h2o_train,\n",
    "                                           build_tree_one_node = bton,\n",
    "                                           binomial_double_trees = bdt,\n",
    "                                           balance_classes = bc,\n",
    "                                           max_depth = md,\n",
    "                                           seed = 213,\n",
    "                                           max_after_balance_size = mabs,\n",
    "                                           ntrees = 50)\n",
    "            #h2o_test = h2o.upload_file(path = \"/Users/Alimantu/Documents/Python/RossmanStoreSales/Data/ready_test_1.csv\")\n",
    "            h2o_test = h2o.upload_file(path = \"/Users/Alimantu/Documents/Python/RossmanStoreSales/tmp/test_1.csv\")\n",
    "            predictions = h2o_rf.predict(h2o_test[features])\n",
    "            h2o_tmp = pd.DataFrame(np.arange(1, len(X_test) + 1), columns=[\"Id\"])\n",
    "            h2o_tmp[\"Sales1\"] = np.exp(predictions.as_data_frame().values) - 1\n",
    "            indices = test.Open == 0\n",
    "            h2o_tmp.loc[indices, \"Sales1\"] = 0\n",
    "            indices = h2o_tmp.Sales1 < 0\n",
    "            h2o_tmp.loc[indices, \"Sales1\"] = 0\n",
    "            h2o_tmp[\"Sales2\"] = X_test.Sales.values\n",
    "            res = sum(h2o_tmp.apply(lambda x: ((x.Sales1 - x.Sales2) ** 2) ** 0.5, axis = 1)) / len(h2o_tmp) / np.mean(X_test.Sales)\n",
    "            commit = \"For used mabs = \" + str(mabs) + \", bc = \" + str(bc) + \", md = \" + str(md)\n",
    "            if (res < best_res):\n",
    "                best_res = res\n",
    "                best_commit = commit\n",
    "            print commit\n",
    "            print \"Rmse = \" + str(res)\n",
    "print \"===============================\"\n",
    "print best_commit\n",
    "print best_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test = pd.read_csv('Data/ready_test_1.csv')\n",
    "# features = ['Open', 'Promo', 'Store', 'DayOfWeek', 'CompDist', 'Day',\n",
    "#        'Month', 'Year', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "#        'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC']\n",
    "def h2o_rf(train_path, test_path, features, default_seed = 213, n_est = 500):\n",
    "    h2o.init(start_h2o = True)\n",
    "#     h2o_train = h2o.upload_file(path = \"/Users/Alimantu/Documents/Python/RossmanStoreSales/Data/ready_data_1.csv\")\n",
    "    h2o_train = h2o.upload_file(path = train_path)\n",
    "\n",
    "    # With this values the final rmse on crossval score was about 0.265486806381\n",
    "    bton = True\n",
    "    bdt = True\n",
    "    best_res = 1\n",
    "    mabs = 3\n",
    "    bc = False\n",
    "    md = 20\n",
    "\n",
    "    h2o_rf = h2o.h2o.random_forest(x = h2o_train[features],\n",
    "                                   y = h2o_train[\"LogSales\"],  \n",
    "                                   training_frame=h2o_train,\n",
    "                                   build_tree_one_node = True,\n",
    "                                   binomial_double_trees = True,\n",
    "                                   balance_classes = False,\n",
    "                                   max_depth = 20,\n",
    "                                   seed = default_seed,\n",
    "                                   max_after_balance_size = 3,\n",
    "                                   ntrees = n_est)\n",
    "\n",
    "    h2o_test = h2o.upload_file(path = test_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    predictions = h2o_rf.predict(h2o_test[features])\n",
    "    h2o_submission = pd.DataFrame(np.arange(1, len(test) + 1), columns=[\"Id\"])\n",
    "    h2o_submission[\"Sales\"] = np.exp(predictions.as_data_frame().values) - 1\n",
    "    indices = test.Open == 0\n",
    "    h2o_submission.loc[indices, \"Sales\"] = 0\n",
    "    indices = h2o_submission.Sales < 0\n",
    "    h2o_submission.loc[indices, \"Sales\"] = 0\n",
    "    # h2o_submission.to_csv(\"Result/h2o_rf.csv\", index=False)\n",
    "    return h2o_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def h2o_rf_model_gen(train, features, n_est = 500):\n",
    "    h2o.init(start_h2o = True)\n",
    "    train.to_csv(\"tmp/train_tmp.csv\")\n",
    "    h2o_train = h2o.upload_file(\"/Users/Alimantu/Documents/Python/RossmanStoreSales/tmp/train_tmp.csv\")\n",
    "    h2o_rf = h2o.h2o.random_forest(x = h2o_train[features],\n",
    "                                   y = h2o_train[\"LogSales\"],  \n",
    "                                   training_frame=h2o_train,\n",
    "                                   build_tree_one_node = True,\n",
    "                                   binomial_double_trees = True,\n",
    "                                   balance_classes = False,\n",
    "                                   max_depth = 20,\n",
    "                                   seed = 213,\n",
    "                                   max_after_balance_size = 3,\n",
    "                                   ntrees = n_est)\n",
    "    return h2o_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning (h2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train = pd.read_csv('Data/ready_data_cleared_zero_sales_1.csv')\n",
    "train = pd.read_csv('Data/ready_data_1.csv')\n",
    "test = pd.read_csv('Data/ready_test_1.csv')\n",
    "X_train, X_test = cross_validation.train_test_split(train, test_size=0.01)\n",
    "train2 = X_train[X_train.Sales != 0]\n",
    "train2.to_csv(\"tmp/data_zr_1.csv\")\n",
    "X_train.to_csv(\"tmp/data_1.csv\")\n",
    "X_test.to_csv(\"tmp/test_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Data/ready_data_without_outliers_2.csv\")\n",
    "X_train, X_test = cross_validation.train_test_split(train, test_size=0.01)\n",
    "X_train.to_csv(\"tmp/data_1.csv\")\n",
    "X_test.to_csv(\"tmp/test_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def h2o_dl_model_gen(train, features, hidden_ = [300, 300, 300, 300, 300], epochs_ = 120):\n",
    "    h2o.init(start_h2o = True)\n",
    "    train.to_csv(\"tmp/train_tmp.csv\")\n",
    "    h2o_train = h2o.upload_file(\"/Users/Alimantu/Documents/Python/RossmanStoreSales/tmp/train_tmp.csv\")\n",
    "    h2o_dl = h2o.deeplearning(x = h2o_train[features], \n",
    "                              y = h2o_train[\"LogSales\"],\n",
    "                              training_frame=h2o_train, \n",
    "                              activation = \"Tanh\",\n",
    "                              epochs = epochs_, \n",
    "                              hidden = hidden_,\n",
    "                              seed = 213,\n",
    "                              loss = \"MeanSquare\")\n",
    "    return h2o_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('Data/ready_test_1.csv')\n",
    "# features = ['Open', 'Promo', 'Store', 'DayOfWeek', 'CompDist', 'Day',\n",
    "#        'Month', 'Year', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "#        'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC']\n",
    "features = ['Open', 'Store', 'DayOfWeek', 'YearWeek', 'Promo', 'Day',\n",
    "       'Month', 'Year', 'SchoolHoliday', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "       'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC']\n",
    "h2o.init(start_h2o = True)\n",
    "# h2o_train_new = h2o.upload_file(path = \"/Users/Alimantu/Documents/Python/RossmanStoreSales/tmp/data_1.csv\")\n",
    "h2o_train = h2o.upload_file(path = \"/Users/Alimantu/Documents/Python/RossmanStoreSales/tmp/data_1.csv\")\n",
    "# h2o_train_zr = h2o.upload_file(path = \"/Users/Alimantu/Documents/Python/RossmanStoreSales/tmp/data_zr_1.csv\")\n",
    "h2o_test = h2o.upload_file(path = \"/Users/Alimantu/Documents/Python/RossmanStoreSales/tmp/test_1.csv\")\n",
    "h2o_tmp = pd.DataFrame(np.arange(1, len(X_test) + 1), columns=[\"Id\"])\n",
    "h2o_tmp[\"Sales2\"] = X_test.Sales.values\n",
    "best_res = 100\n",
    "best_commit = \"Very bad \\(>.<)/\"\n",
    "\n",
    "#searching for best activation function\n",
    "#for active in [\"Tanh\", \"TanhWithDropout\", \"Rectifier\", \"RectifierWithDropout\", \"Maxout\", \"MaxoutWithDropout\"]:\n",
    "active = \"Tanh\"\n",
    "seed_ = 213\n",
    "loss_ = \"MeanSquare\"\n",
    "\n",
    "epochs_ = 120\n",
    "#for epochs_ in [100, 1000, 5000, 10000]:\n",
    "iter_ = 0\n",
    "# for h2o_train in [h2o_train_new, h2o_train_zr]:\n",
    "#     iter_ += 1\n",
    "#for hidden_ in [[300, 300, 300, 300, 300], [200, 200, 200, 200, 200, 200]]:\n",
    "for hidden_ in [[50, 50, 50], [50, 50, 50, 50]]:\n",
    "    h2o_dl = h2o.deeplearning(x = h2o_train[features], \n",
    "                              y = h2o_train[\"LogSales\"],\n",
    "                              training_frame=h2o_train, \n",
    "                              activation = active,\n",
    "                              epochs = epochs_, \n",
    "                              hidden = hidden_,\n",
    "                              seed = seed_,\n",
    "                              loss = loss_)\n",
    "    predictions = h2o_dl.predict(h2o_test[features])\n",
    "    h2o_tmp[\"Sales1\"] = np.exp(predictions.as_data_frame().values) - 1\n",
    "    indices = X_test.Open == 0\n",
    "    indices.index = np.arange(len(X_test))\n",
    "    h2o_tmp.loc[indices, \"Sales1\"] = 0\n",
    "    indices = h2o_tmp.Sales1 < 0\n",
    "    h2o_tmp.loc[indices, \"Sales1\"] = 0\n",
    "    res = sum(h2o_tmp.apply(lambda x: ((x.Sales1 - x.Sales2) ** 2) ** 0.5, axis = 1)) / len(h2o_tmp) / np.mean(X_test.Sales)\n",
    "    commit = \"For used hidden_ = \" + str(hidden_) + \", iter = \" + str(iter_)\n",
    "    if (res < best_res):\n",
    "        best_res = res\n",
    "        best_commit = commit\n",
    "    print commit\n",
    "    print \"Rmse = \" + str(res)\n",
    "print \"===============================\"\n",
    "print best_commit\n",
    "print best_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For used epochs_ = 100, hidden_ = [50, 50, 50, 50]   \n",
    "Rmse = 0.12393553862   \n",
    "For used epochs_ = 100, hidden_ = [70, 70, 70, 70]   \n",
    "Rmse = 0.108514525843   \n",
    "For used epochs_ = 100, hidden_ = [30, 30, 30, 30, 30]   \n",
    "Rmse = 0.150923171566\n",
    "\n",
    "For used hidden_ = [100, 100, 100, 100, 100], iter = 0   \n",
    "Rmse = 0.103013410371 (result with hint .17870)   \n",
    "For used hidden_ = [300, 300, 300, 300, 300], iter = 0   \n",
    "Rmse = 0.0912935961785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('Data/ready_test_1.csv')\n",
    "features = ['Open', 'Promo', 'Store', 'DayOfWeek', 'CompDist', 'Day',\n",
    "       'Month', 'Year', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "       'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC']\n",
    "h2o.init(start_h2o = True)\n",
    "h2o_train = h2o.upload_file(path = \"/Users/Alimantu/Documents/Python/RossmanStoreSales/Data/ready_data_1.csv\")\n",
    "\n",
    "h2o_rf = h2o.deeplearning(x = h2o_train[features],\n",
    "                          y = h2o_train[\"LogSales\"],\n",
    "                          training_frame=h2o_train,\n",
    "                         )\n",
    "\n",
    "\n",
    "h2o_test = h2o.upload_file(path = \"/Users/Alimantu/Documents/Python/RossmanStoreSales/Data/ready_test_1.csv\")\n",
    "predictions = h2o_rf.predict(h2o_test[features])\n",
    "h2o_submission = pd.DataFrame(np.arange(1, len(test) + 1), columns=[\"Id\"])\n",
    "h2o_submission[\"Sales\"] = np.exp(predictions.as_data_frame().values) - 1\n",
    "indices = test.Open == 0\n",
    "h2o_submission.loc[indices, \"Sales\"] = 0\n",
    "indices = h2o_submission.Sales < 0\n",
    "h2o_submission.loc[indices, \"Sales\"] = 0\n",
    "h2o_submission.to_csv(\"Result/h2o_rf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresures = ['Open', 'Store', 'DayOfWeek', 'YearWeek', 'Promo', 'Day',\n",
    "       'Month', 'Year', 'SchoolHoliday', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "       'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC']\n",
    "train = pd.read_csv(\"Data/ready_data_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skl_rf_model_gen(train, features, n_est = 500):\n",
    "    skl_rf = RandomForestRegressor(random_state=213\n",
    "                                   , n_estimators = 20\n",
    "                                   , criterion = 'mse'\n",
    "                                   , min_samples_split = 5\n",
    "                                   , min_samples_leaf = 1\n",
    "                                   , max_depth = 30\n",
    "                                   , min_weight_fraction_leaf = 0.\n",
    "                                   , max_leaf_nodes = None\n",
    "                                   , bootstrap = True\n",
    "                                   , oob_score = True\n",
    "                                   , n_jobs = -1)\n",
    "    skl_rf.fit(train.loc[:, features], train.loc[:, \"LogSales\"])\n",
    "    return skl_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search_cv = gs.GridSearchCV(RandomForestRegressor(random_state=213\n",
    "                                                       , n_estimators = 20\n",
    "                                                       , criterion = 'mse'\n",
    "                                                       , min_samples_split = 5\n",
    "                                                       , min_samples_leaf = 1\n",
    "                                                       , max_depth = 30\n",
    "                                                       , min_weight_fraction_leaf = 0.\n",
    "                                                       , max_leaf_nodes = None\n",
    "                                                       , bootstrap = True\n",
    "                                                       , oob_score = True\n",
    "                                                      ), {\n",
    "#         'max_depth': (10, 30)\n",
    "        'oob_score' : (True, False)\n",
    "#        'loss' : ('ls', 'lad', 'huber', 'quantile')#,\n",
    "#        'learning_rate' : (0.0001, 0.01, 0.1, 1, 10),\n",
    "#        'n_estimators' : (10, 50, 100),\n",
    "#        'n_estimators' : (10, 20)\n",
    "#        'min_samples_leaf' : (1, 2),\n",
    "#        'min_samples_split': (10, 20)\n",
    "    },\n",
    "                                 scoring='mean_squared_error', n_jobs = -1, cv=4, verbose=10)\n",
    "grid_search_cv.fit(train.loc[:, features], train.loc[:, \"LogSales\"])\n",
    "print(-grid_search_cv.best_score_) ** 0.5 / np.mean(train.loc[:, \"LogSales\"])\n",
    "grid_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['Open', 'Store', 'DayOfWeek', 'YearWeek', 'Promo', 'Day',\n",
    "       'Month', 'Year', 'SchoolHoliday', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "       'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC']\n",
    "train = pd.read_csv(\"Data/ready_data_without_outliers_1.csv\")\n",
    "skl_rf = RandomForestRegressor(n_estimators = 1000, min_samples_leaf=1, n_jobs = -1, random_state=213, criterion='mse',\n",
    "                              min_samples_split=20, verbose = True)\n",
    "skl_rf.fit(train.loc[:, features], train.loc[:, \"LogSales\"])\n",
    "test = pd.read_csv(\"Data/ready_test_1.csv\")\n",
    "submission = pd.DataFrame(np.arange(1, len(test) + 1), columns=[\"Id\"])\n",
    "submission[\"Sales\"] = (np.exp(skl_rf.predict(test.loc[:, features])) - 1) * 0.985\n",
    "indices = test.Open == 0\n",
    "submission.loc[indices, \"Sales\"] = 0\n",
    "indices = submission.Sales < 0\n",
    "submission.loc[indices, \"Sales\"] = 0\n",
    "submission.to_csv(\"Result/Skl_rf_1000_wo_hint.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['Open', 'Store', 'DayOfWeek', 'YearWeek', 'Promo', 'Day',\n",
    "       'Month', 'Year', 'SchoolHoliday', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "       'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC']\n",
    "train = pd.read_csv(\"Data/ready_data_without_outliers_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_depth = 10\n",
    "grid_search_cv = gs.GridSearchCV(GradientBoostingRegressor(random_state=213\n",
    "                                                           , loss = 'ls'\n",
    "                                                           , n_estimators = 10\n",
    "                                                           , min_samples_leaf = 1\n",
    "                                                           , min_samples_split = 1\n",
    "                                                           , learning_rate = 1\n",
    "                                                           , max_leaf_nodes = -1\n",
    "                                                          ), {\n",
    "        'max_depth': (10, 30)\n",
    "#        'loss' : ('ls', 'lad', 'huber', 'quantile')#,\n",
    "#        'learning_rate' : (0.0001, 0.01, 0.1, 1, 10),\n",
    "#        'n_estimators' : (10, 50, 100),\n",
    "#        'n_estimators' : (10, 20)\n",
    "#        'min_samples_leaf' : (1, 2),\n",
    "#        'min_samples_split': (10, 20)\n",
    "    },\n",
    "                                 scoring='mean_squared_error', n_jobs = -1, cv=4, verbose=10)\n",
    "grid_search_cv.fit(train.loc[:, features], train.loc[:, \"LogSales\"])\n",
    "print(-grid_search_cv.best_score_) ** 0.5 / np.mean(train.loc[:, \"LogSales\"])\n",
    "grid_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def skl_gb(train, test, features, n_est = 300):\n",
    "    skl_gb = GradientBoostingRegressor(random_state=213\n",
    "                                       , loss = 'ls'\n",
    "                                       , min_samples_leaf = 1\n",
    "                                       , min_samples_split = 1\n",
    "                                       , learning_rate = 1\n",
    "                                       , max_leaf_nodes = -1\n",
    "                                       , n_estimators = n_est)\n",
    "    skl_gb.fit(train.loc[:, features], train.loc[:, \"LogSales\"])\n",
    "    result = pd.DataFrame(np.arange(1, len(test) + 1), columns=[\"Id\"])\n",
    "    result[\"LogSales\"] = skl_gb.predict(test.loc[:, features])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skl_gb_model_gen(train, features, n_est = 300):\n",
    "    skl_gb = GradientBoostingRegressor(random_state=213\n",
    "                                       , loss = 'ls'\n",
    "                                       , min_samples_leaf = 1\n",
    "                                       , min_samples_split = 1\n",
    "                                       , learning_rate = 1\n",
    "                                       , max_leaf_nodes = -1\n",
    "                                       , n_estimators = n_est)\n",
    "    skl_gb.fit(train.loc[:, features], train.loc[:, \"LogSales\"])\n",
    "    return skl_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Data/ready_data_1.csv\")\n",
    "res = remove_outliers(train)\n",
    "res.to_csv(\"Data/ready_data_without_outliers_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is the pain begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features = ['Open', 'Store', 'DayOfWeek', 'YearWeek', 'Promo', 'CompDist', 'Day',\n",
    "#        'Month', 'Year', 'SchoolHoliday', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "#        'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC']\n",
    "features = ['Open', 'Store', 'DayOfWeek', 'YearWeek', 'Promo', 'Day',\n",
    "       'Month', 'Year', 'SchoolHoliday', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "       'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_submit = h2o_rf(\"/Users/Alimantu/Documents/Python/RossmanStoreSales/Data/ready_data_without_outliers_2.csv\",\n",
    "      \"/Users/Alimantu/Documents/Python/RossmanStoreSales/Data/ready_test_1.csv\", features)\n",
    "rf_submit.Sales = 0.985 * rf_submit.Sales\n",
    "rf_submit.to_csv(\"Result/h2o_rf_500_wo_hint.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['Open', 'Store', 'DayOfWeek', 'YearWeek', 'Promo', 'Day',\n",
    "       'Month', 'Year', 'SchoolHoliday', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "       'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC']\n",
    "train = pd.read_csv(\"Data/ready_data_without_outliers_3.csv\")\n",
    "test = pd.read_csv(\"Data/ready_test_1.csv\")\n",
    "xgboost_submit = xgboost(train, test, features, n_est=500)\n",
    "xgboost_submit.to_csv(\"Result/xgb_500_wo.csv\",index=False)\n",
    "xgboost_submit.Sales = xgboost_submit.Sales * 0.985\n",
    "xgboost_submit.to_csv(\"Result/xgb_500_wo_hint.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['Open', 'Store', 'DayOfWeek', 'YearWeek', 'Promo', 'Day',\n",
    "       'Month', 'Year', 'SchoolHoliday', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "       'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC']\n",
    "train = pd.read_csv(\"Data/ready_data_without_outliers_3.csv\")\n",
    "test = pd.read_csv(\"Data/ready_test_1.csv\")\n",
    "skl_gb_submit = skl_gb(train = train, test = test, features = features)\n",
    "# skl_gb_submit.to_csv(\"Result/skl_gb_300_wo.csv\",index=False)\n",
    "# skl_gb_submit.Sales = xgboost_submit.Sales * 0.985\n",
    "# skl_gb_submit.to_csv(\"Result/skl_gb_300_wo_hint.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skl_gb_submit_2 = pd.DataFrame({ \"Id\" :skl_gb_submit.Id.values})\n",
    "skl_gb_submit_2[\"Sales\"] = np.exp(skl_gb_submit[\"LogSales\"]) - 1\n",
    "indices = test.Open == 0\n",
    "skl_gb_submit_2.loc[indices, \"Sales\"] = 0\n",
    "indices = skl_gb_submit_2.Sales < 0\n",
    "skl_gb_submit_2.loc[indices, \"Sales\"] = 0\n",
    "skl_gb_submit_2.head()\n",
    "skl_gb_submit_2.to_csv(\"Result/skl_gb_300_wo.csv\", index=False)\n",
    "skl_gb_submit_2.Sales = skl_gb_submit_2.Sales * 0.985\n",
    "skl_gb_submit_2.head()\n",
    "skl_gb_submit_2.to_csv(\"Result/skl_gb_300_wo_hint.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First ensamble\n",
    "Here I'll use the first edition of the data, without any cutting of the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_1 = ['Open', 'Store', 'DayOfWeek', 'YearWeek', 'Promo', 'Day',\n",
    "       'Month', 'Year', 'SchoolHoliday', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "       'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC']\n",
    "train = pd.read_csv(\"Data/ready_data_1.csv\")\n",
    "test = pd.read_csv(\"Data/ready_test_1.csv\")\n",
    "train_tmp, train_3 = cross_validation.train_test_split(train, test_size=0.2)\n",
    "train_1, train_2 = cross_validation.train_test_split(train_tmp, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o_rf_model_1 = h2o_rf_model_gen(train = train_1, features = features_1, n_est = 1000)\n",
    "h2o_dl_model_1 = h2o_dl_model_gen(train = train_1, features = features_1, hidden_=[70, 70, 70, 70, 70], epochs_=100)\n",
    "skl_gb_model_1 = skl_gb_model_gen(train = train_1, features = features_1, n_est = 500)\n",
    "skl_rf_model_1 = skl_rf_model_gen(train = train_1, features = features_1, n_est = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_model_1 = xgb_model_gen(train = train_1, features = features_1, lambda_=1, n_est = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_2.to_csv('tmp/data_ensamble_1_2.csv')\n",
    "train_2_h2o = h2o.upload_file('/Users/Alimantu/Documents/Python/RossmanStoreSales/tmp/data_ensamble_1_2.csv')\n",
    "predictions = h2o_dl_model_1.predict(train_2_h2o[features_1])\n",
    "train_2.loc[:, \"Pred1\"] = predictions.as_data_frame().values\n",
    "predictions = h2o_rf_model_1.predict(train_2_h2o[features_1])\n",
    "train_2.loc[:, \"Pred2\"] = predictions.as_data_frame().values\n",
    "train_2.loc[:, \"Pred3\"] = skl_gb_model_1.predict(train_2.loc[:, features_1])\n",
    "train_2.loc[:, \"Pred4\"] = skl_rf_model_1.predict(train_2.loc[:, features_1])\n",
    "train_2.loc[:, \"Pred5\"] = xgb_model_1.predict(xgb.DMatrix(train_2[features_1]))\n",
    "train_2.Pred5 = train_2.Pred5.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_2.to_csv('tmp/data_ensamble_2_ready.csv')\n",
    "features_2 = ['Open', 'Store', 'DayOfWeek', 'YearWeek', 'Promo', 'Day',\n",
    "              'Month', 'Year', 'SchoolHoliday', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "              'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC', 'Pred2', 'Pred1',\n",
    "              'Pred3', 'Pred4', 'Pred5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o_rf_model_2 = h2o_rf_model_gen(train = train_2, features = features_2, n_est = 1000)\n",
    "h2o_dl_model_2 = h2o_dl_model_gen(train = train_2, features = features_2, hidden_=[70, 70, 70, 70, 70], epochs_=100)\n",
    "skl_gb_model_2 = skl_gb_model_gen(train = train_2, features = features_2, n_est = 500)\n",
    "skl_rf_model_2 = skl_rf_model_gen(train = train_2, features = features_2, n_est = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_model_2 = xgb_model_gen(train = train_2, features = features_2, lambda_=1, eta=0.04, n_est=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predictions of the first level\n",
    "train_3.to_csv('tmp/data_3_tmp.csv')\n",
    "train_3_h2o = h2o.upload_file('/Users/Alimantu/Documents/Python/RossmanStoreSales/tmp/data_3_tmp.csv')\n",
    "predictions = h2o_dl_model_1.predict(train_3_h2o[features_1])\n",
    "train_3[\"Pred1\"] = predictions.as_data_frame().values\n",
    "predictions = h2o_rf_model_1.predict(train_3_h2o[features_1])\n",
    "train_3[\"Pred2\"] = predictions.as_data_frame().values\n",
    "train_3[\"Pred3\"] = skl_gb_model_1.predict(train_3.loc[:, features_1])\n",
    "train_3[\"Pred4\"] = skl_rf_model_1.predict(train_3.loc[:, features_1])\n",
    "train_3[\"Pred5\"] = xgb_model_1.predict(xgb.DMatrix(train_3[features_1]))\n",
    "train_3.Pred5 = train_3.Pred5.astype(float)\n",
    "\n",
    "# Predictions of the second level\n",
    "train_3.to_csv('tmp/data_3_tmp_2.csv')\n",
    "train_3_h2o = h2o.upload_file('/Users/Alimantu/Documents/Python/RossmanStoreSales/tmp/data_3_tmp_2.csv')\n",
    "predictions = h2o_dl_model_2.predict(train_3_h2o[features_2])\n",
    "train_3[\"Pred6\"] = predictions.as_data_frame().values\n",
    "predictions = h2o_rf_model_2.predict(train_3_h2o[features_2])\n",
    "train_3[\"Pred7\"] = predictions.as_data_frame().values\n",
    "train_3[\"Pred8\"] = skl_gb_model_2.predict(train_3.loc[:, features_2])\n",
    "train_3[\"Pred9\"] = skl_rf_model_2.predict(train_3.loc[:, features_2])\n",
    "train_3[\"Pred10\"] = xgb_model_2.predict(xgb.DMatrix(train_3[features_2]))\n",
    "train_3.Pred10 = train_3.Pred10.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_3.to_csv('tmp/train_3_ready.csv')\n",
    "features_3 = ['Open', 'Store', 'DayOfWeek', 'YearWeek', 'Promo', 'Day',\n",
    "              'Month', 'Year', 'SchoolHoliday', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "              'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC', 'Pred2', 'Pred1',\n",
    "              'Pred3', 'Pred4', 'Pred5', 'Pred6', 'Pred7', 'Pred8', 'Pred9', 'Pred10']\n",
    "# h2o_rf_model_3 = h2o_rf_model_gen(train = train_3, features = features_3, n_est = 1000)\n",
    "# h2o_dl_model_3 = h2o_dl_model_gen(train = train_2, features = features_2, hidden_=[70, 70, 70, 70, 70], epochs_=100)\n",
    "# skl_gb_model_3 = skl_gb_model_gen(train = train_3, features = features_3, n_est = 500)\n",
    "# skl_rf_model_3 = skl_rf_model_gen(train = train_3, features = features_3, n_est = 1000)\n",
    "xgb_model_3 = xgb_model_gen(train = train_3, features = features_3, lambda_=1, eta=0.005, n_est=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_3_new = ['Open', 'Store', 'DayOfWeek', 'YearWeek', 'Promo', 'Day',\n",
    "              'Month', 'Year', 'SchoolHoliday', 'LogCompDist', 'StoreA', 'StoreB',\n",
    "              'StoreC', 'StoreD', 'AssortA', 'AssortB', 'AssortC', 'Pred6', 'Pred7', 'Pred8', 'Pred9', 'Pred10']\n",
    "h2o_rf_model_3_new = h2o_rf_model_gen(train = train_3, features = features_3_new, n_est = 300)\n",
    "# h2o_dl_model_3 = h2o_dl_model_gen(train = train_2, features = features_2, hidden_=[70, 70, 70, 70, 70], epochs_=100)\n",
    "# skl_gb_model_3 = skl_gb_model_gen(train = train_3, features = features_3, n_est = 500)\n",
    "# skl_rf_model_3 = skl_rf_model_gen(train = train_3, features = features_3, n_est = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_model_3_new_2 = xgb_model_gen(train = train_3, features = features_3_new, lambda_=1, eta=0.001, n_est=7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('Data/ready_test_1.csv')\n",
    "# Predictions of the first level\n",
    "test.to_csv('tmp/test_tmp.csv')\n",
    "test_h2o = h2o.upload_file('/Users/Alimantu/Documents/Python/RossmanStoreSales/tmp/test_tmp.csv')\n",
    "predictions = h2o_dl_model_1.predict(test_h2o[features_1])\n",
    "test[\"Pred1\"] = predictions.as_data_frame().values\n",
    "predictions = h2o_rf_model_1.predict(test_h2o[features_1])\n",
    "test[\"Pred2\"] = predictions.as_data_frame().values\n",
    "test[\"Pred3\"] = skl_gb_model_1.predict(test.loc[:, features_1])\n",
    "test[\"Pred4\"] = skl_rf_model_1.predict(test.loc[:, features_1])\n",
    "test[\"Pred5\"] = xgb_model_1.predict(xgb.DMatrix(test[features_1]))\n",
    "test.Pred5 = test.Pred5.astype(float)\n",
    "\n",
    "# Predictions of the second level\n",
    "test.to_csv('tmp/test_tmp_2.csv')\n",
    "test_h2o = h2o.upload_file('/Users/Alimantu/Documents/Python/RossmanStoreSales/tmp/test_tmp_2.csv')\n",
    "predictions = h2o_dl_model_2.predict(test_h2o[features_2])\n",
    "test[\"Pred6\"] = predictions.as_data_frame().values\n",
    "predictions = h2o_rf_model_2.predict(test_h2o[features_2])\n",
    "test[\"Pred7\"] = predictions.as_data_frame().values\n",
    "test[\"Pred8\"] = skl_gb_model_2.predict(test.loc[:, features_2])\n",
    "test[\"Pred9\"] = skl_rf_model_2.predict(test.loc[:, features_2])\n",
    "test[\"Pred10\"] = xgb_model_2.predict(xgb.DMatrix(test[features_2]))\n",
    "test.Pred10 = test.Pred10.astype(float)\n",
    "\n",
    "# Final predict\n",
    "result = pd.DataFrame(np.arange(1, len(test) + 1), columns=[\"Id\"])\n",
    "result[\"Sales\"] = np.exp(xgb_model_3.predict(xgb.DMatrix(test[features_3]))) - 1\n",
    "result.to_csv(\"Result/ensable_2.csv\", index=False)\n",
    "result.Sales = result.Sales * 0.985\n",
    "result.to_csv(\"Result/ensable_2_hint.csv\", index=False)\n",
    "indices = test.Open == 0\n",
    "result.loc[indices, \"Sales\"] = 0\n",
    "indices = result.Sales < 0\n",
    "result.loc[indices, \"Sales\"] = 0\n",
    "result.to_csv(\"Result/ensable_2_corr_hint.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.to_csv('tmp/test_tmp_3.csv')\n",
    "test_h2o = h2o.upload_file('/Users/Alimantu/Documents/Python/RossmanStoreSales/tmp/test_tmp_3.csv')\n",
    "predictions = h2o_rf_model_3_new.predict(test_h2o[features_3_new])\n",
    "result[\"Sales\"] = np.exp(predictions.as_data_frame().values) - 1\n",
    "indices = test.Open == 0\n",
    "result.loc[indices, \"Sales\"] = 0\n",
    "indices = result.Sales < 0\n",
    "result.loc[indices, \"Sales\"] = 0\n",
    "result.Sales = result.Sales * 0.985\n",
    "result.to_csv(\"Result/ensable_sum_h2o_rf_hint.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result[\"Sales\"] = np.exp(xgb_model_2.predict(xgb.DMatrix(test[features_2]))) - 1\n",
    "result.Sales = result.Sales * 0.985\n",
    "indices = test.Open == 0\n",
    "result.loc[indices, \"Sales\"] = 0\n",
    "indices = result.Sales < 0\n",
    "result.loc[indices, \"Sales\"] = 0\n",
    "result.to_csv(\"Result/ensable_1_summ_xgb_hint.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = pd.read_csv('tmp/Goose_hint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = test.Open == 0\n",
    "res.loc[indices, \"Sales\"] = 0\n",
    "indices = res.Sales < 0\n",
    "res.loc[indices, \"Sales\"] = 0\n",
    "res.to_csv('Result/Goose_fixed_hint.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
